# NTSB Datasets - Custom Claude Commands

Custom slash commands for development workflow automation with [Claude Code](https://claude.com/claude-code).

**Location:** `.claude/commands/`
**Count:** 12 commands (6 existing + 6 new)
**Purpose:** Database project workflow automation
**Adapted From:** ProRT-IP custom commands + Database-specific originals
**Updated:** 2025-11-06

---

## Overview

This directory contains 12 custom slash commands designed to streamline the NTSB Aviation Database development workflow. These commands automate data loading, validation, quality assessment, backup/restore, performance testing, sample exports, version control workflows, and daily consolidation tasks.

### Command Categories

1. **Data Operations** (3 commands) ‚≠ê NEW
   - `/load-data` - Intelligent data loading wrapper with validation
   - `/data-coverage` - Comprehensive coverage analysis (temporal, geographic, aircraft)
   - `/export-sample` - Export filtered sample datasets for testing/sharing

2. **Data Quality** (2 commands)
   - `/validate-schema` - Comprehensive database integrity checks
   - `/data-quality` - Data quality dashboard with NULL analysis and outlier detection ‚≠ê NEW

3. **Database Maintenance** (3 commands)
   - `/cleanup-staging` - Clear staging tables and vacuum database
   - `/refresh-mvs` - Refresh materialized views
   - `/backup-db` - Automated database backups with compression ‚≠ê NEW

4. **Performance Testing** (1 command)
   - `/benchmark` - Query performance measurement and reporting

5. **Version Control** (1 command) ‚≠ê NEW
   - `/stage-commit` - Comprehensive pre-commit workflow with quality checks

6. **Project Management** (1 command)
   - `/sprint-status` - Show current sprint and database status

7. **Daily Consolidation** (1 command)
   - `/daily-log` - End-of-day comprehensive file preservation and documentation

**‚≠ê NEW**: 6 new commands created for enhanced database workflow automation

---

## Quick Command Reference

| Command | Category | Time | Purpose |
|---------|----------|------|---------|
| `/load-data` | Data Ops | 5-15 min | Load MDB files with validation ‚≠ê |
| `/data-coverage` | Data Ops | 5-10 min | Analyze data coverage and gaps ‚≠ê |
| `/export-sample` | Data Ops | 2-5 min | Export sample datasets ‚≠ê |
| `/validate-schema` | Quality | 5-10 min | Database integrity checks |
| `/data-quality` | Quality | 5-10 min | Quality dashboard with outliers ‚≠ê |
| `/backup-db` | Maintenance | 2-5 min | Create compressed backups ‚≠ê |
| `/cleanup-staging` | Maintenance | 5-10 min | Clear staging, vacuum DB |
| `/refresh-mvs` | Maintenance | 1-2 min | Update materialized views |
| `/benchmark` | Performance | 5-10 min | Query performance testing |
| `/stage-commit` | Version Control | 5-15 min | Pre-commit quality workflow ‚≠ê |
| `/sprint-status` | Management | <1 min | Project status overview |
| `/daily-log` | Management | 80 min | End-of-day consolidation |

---

## NEW COMMANDS DOCUMENTATION

### 1. `/load-data` - Intelligent Data Loading Wrapper ‚≠ê

**Purpose:** Safe, guided data loading with automated validation and reporting

**Features:**
- Interactive source selection (avall.mdb, Pre2008.mdb, PRE1982.MDB)
- Load tracking checks to prevent duplicate loads
- Automatic Python environment activation
- Post-load validation (duplicates, orphans, integrity)
- Automatic materialized view refresh
- Comprehensive load report generation

**Usage:**
```bash
/load-data                    # Interactive mode
/load-data avall.mdb          # Load current database
/load-data Pre2008.mdb        # Load historical database
/load-data --dry-run          # Check source without loading
/load-data --force            # Skip load_tracking confirmation
```

**When to Use:**
- Monthly updates from NTSB (avall.mdb)
- First-time historical data integration (Pre2008.mdb)
- After database reset
- When setting up new environment

**Output:**
- Real-time progress updates
- Pre/post load state comparison
- Validation results
- Load report: `/tmp/NTSB_Datasets/load_logs/load_report_*.md`

**Safety Features:**
- Load tracking prevents duplicate loads
- Prompts for historical database loads
- Automatic validation after load
- Comprehensive error handling

---

### 2. `/data-coverage` - Comprehensive Coverage Analysis ‚≠ê

**Purpose:** Analyze data coverage across temporal, geographic, and categorical dimensions

**Analysis Dimensions:**
- **Temporal:** Year range, gaps, decade distribution, trends
- **Geographic:** State coverage, coordinate completeness, bounding box
- **Aircraft:** Type distribution, make/model diversity
- **Findings:** Investigation code frequency, probable causes
- **Phase of Operation:** Accident phase distribution
- **Data Quality:** Completeness scores, validity checks

**Usage:**
```bash
/data-coverage                    # Full coverage analysis
/data-coverage temporal           # Year range and gaps only
/data-coverage geographic         # State and coordinate analysis
/data-coverage --geojson          # Generate GeoJSON for mapping
```

**When to Use:**
- After loading new data sources
- Monthly (to track coverage growth)
- Before major analysis projects
- When planning data enrichment efforts
- To understand data limitations

**Output:**
- Comprehensive coverage report: `/tmp/NTSB_Datasets/coverage_analysis/coverage_report_*.md`
- GeoJSON file (optional): `/tmp/NTSB_Datasets/coverage_analysis/accidents_geojson_*.json`
- Gap identification and recommendations

**Key Insights:**
- Temporal gaps (missing years)
- Geographic distribution (state coverage)
- Coordinate completeness percentage
- Aircraft type diversity metrics
- Finding code frequency analysis

---

### 3. `/export-sample` - Sample Data Export ‚≠ê

**Purpose:** Export filtered sample datasets for testing, sharing, and demos

**Sample Sizes:**
- **tiny** - 100 events (~2,000 rows, <1MB)
- **small** - 1,000 events (~20,000 rows, ~5MB)
- **medium** - 10,000 events (~200,000 rows, ~50MB)
- **custom** - User-specified count

**Filters:**
- Year range (e.g., 2020-2023)
- States (e.g., CA,TX,FL)
- Fatal accidents only
- Aircraft type (e.g., Cessna)
- Recent (last 5 years)

**Usage:**
```bash
/export-sample                    # Interactive mode
/export-sample tiny               # Export 100 events
/export-sample --count 500        # Export 500 events
/export-sample --year 2020-2023   # Filter by year
/export-sample --state CA,TX      # Filter by states
/export-sample --fatal            # Fatal accidents only
```

**When to Use:**
- Testing ETL pipelines
- Documentation examples
- Demos and workshops
- Sharing with colleagues
- Quick analysis without full database

**Output:**
- ZIP archive with 9 CSV files + metadata
- Preserves referential integrity
- METADATA.md with comprehensive documentation
- Location: `/tmp/NTSB_Datasets/exports/ntsb_sample_*.zip`

**Exported Tables:**
- events.csv, aircraft.csv, flight_crew.csv
- injury.csv, findings.csv, narratives.csv
- engines.csv, events_sequence.csv, ntsb_admin.csv

---

### 4. `/data-quality` - Data Quality Dashboard ‚≠ê

**Purpose:** Comprehensive data quality assessment with NULL analysis, outlier detection, and integrity checks

**Quality Dimensions:**
- **Completeness:** NULL value analysis by field
- **Validity:** Outlier detection (coordinates, dates, ages)
- **Accuracy:** Data range validation
- **Consistency:** Referential integrity checks
- **Uniqueness:** Duplicate detection (beyond events)

**Analysis Types:**
- NULL values across all tables and fields
- Invalid coordinates (out of range)
- Date outliers (future dates, very old dates)
- Crew age outliers (<10 or >120 years)
- Year distribution outliers (unusual spikes/drops)
- Duplicate events, narratives, aircraft registrations
- Orphaned records (referential integrity)
- Completeness scores by table and field

**Usage:**
```bash
/data-quality                    # Full quality assessment
/data-quality nulls              # NULL value analysis only
/data-quality outliers           # Outlier detection only
/data-quality duplicates         # Duplicate detection only
/data-quality integrity          # Referential integrity only
/data-quality --html             # Generate HTML dashboard
```

**When to Use:**
- After every data load
- Weekly automated runs
- Before major analysis projects
- Monthly quality reports
- When data issues suspected

**Output:**
- Comprehensive quality report: `/tmp/NTSB_Datasets/data_quality_*/data_quality_report.md`
- HTML dashboard (optional): `/tmp/NTSB_Datasets/data_quality_*/data_quality_dashboard.html`
- Analysis files (nulls, outliers, duplicates, integrity, completeness)

**Key Metrics:**
- Overall quality grade (A+, A, B+, etc.)
- Field completeness percentages
- Outlier counts and examples
- Duplicate detection results
- Referential integrity status

---

### 5. `/backup-db` - Automated Database Backup ‚≠ê

**Purpose:** Create timestamped PostgreSQL backups with compression and verification

**Backup Types:**
- **full** - Complete database (schema + data) - Default
- **schema** - Schema only (tables, indexes, functions)
- **data** - Data only (no schema)

**Features:**
- Automated pg_dump execution
- Gzip compression (10-20% of uncompressed size)
- Backup verification (integrity checks)
- Metadata file generation
- Retention policy management
- MD5 checksum calculation

**Usage:**
```bash
/backup-db                       # Full backup (default)
/backup-db schema                # Schema only
/backup-db data                  # Data only
/backup-db --name my_backup      # Custom name prefix
/backup-db --keep 10             # Keep only 10 most recent
/backup-db --verify              # Verify backup after creation
```

**When to Use:**
- Daily automated backups (cron/systemd)
- Before major operations (schema changes, large loads)
- Before testing destructive operations
- Weekly/monthly archival backups
- Disaster recovery preparation

**Output:**
- Compressed backup: `backups/ntsb_aviation_[TYPE]_[TIMESTAMP].sql.gz`
- Metadata file: `backups/ntsb_aviation_[TYPE]_[TIMESTAMP].metadata.txt`
- Restore commands and verification instructions

**Performance:**
- Full backup (966 MB database): 2-3 minutes
- Compressed size: ~100-200 MB (~10-20% of uncompressed)
- Schema-only: <30 seconds

**Safety:**
- Read-only operation (no database modifications)
- No locking (safe during active use)
- Atomic operation (backup or fail cleanly)

---

### 6. `/stage-commit` - Comprehensive Pre-Commit Workflow ‚≠ê

**Purpose:** Automated pre-commit workflow with quality checks, documentation updates, and comprehensive commit messages

**Features:**
- **12 Execution Phases:** Analysis ‚Üí Quality ‚Üí Environment ‚Üí Gitignore ‚Üí Safety ‚Üí Documentation ‚Üí Memory Bank ‚Üí Cross-Reference ‚Üí Verification ‚Üí Staging ‚Üí Commit ‚Üí Review
- **File-Type Aware Quality Checks:**
  - Python: ruff linting and formatting
  - SQL: Syntax validation, dangerous operation detection
  - Markdown: Link validation, code block checks
  - Bash: shellcheck validation
- **Database-Specific Safety:**
  - Schema change detection with backup recommendations
  - ETL script validation
  - Sensitive file blocking (credentials, large files)
  - Data file prevention (.csv, .mdb)
- **Documentation Automation:**
  - README.md database metrics updates
  - CHANGELOG.md entry generation
  - Memory bank optimization (CLAUDE.local.md)
  - Cross-reference validation
- **Conventional Commits:**
  - 11 commit types (data, schema, query, etl, docs, feat, fix, perf, refactor, test, chore)
  - Structured commit messages with metadata
  - Database state tracking in commit messages
- **Safety Features:**
  - User confirmation required before commit
  - Abort on quality check failures
  - Sensitive file detection and blocking
  - Post-commit recommendations

**Usage:**
```bash
/stage-commit                    # Full workflow (all phases)
```

**When to Use:**
- **Always:** Before every commit (make this your standard workflow)
- **Required:** Before schema changes, ETL updates, major refactoring
- **Essential:** Before sprint completion commits

**Execution Flow:**
1. **Phase 1-2:** Analyze changes, validate code quality (Python, SQL, Markdown, Bash)
2. **Phase 3-5:** Check environment, maintain .gitignore, database safety checks
3. **Phase 6-8:** Update documentation (README, CHANGELOG, memory banks, cross-refs)
4. **Phase 9-10:** Final verification, stage all changes
5. **Phase 11-12:** Create comprehensive commit message, user confirmation

**Commit Message Structure:**
```
<type>(<scope>): <summary>

<detailed description>

## Changes Made
- Specific changes with context

## Impact
- Performance/Data/Quality/Features/Fixes

## Files Modified
- path/to/file - description

## Database State (if applicable)
- Database size, event count, coverage

## Testing/Validation
- Schema validation, data quality, benchmarks

## Documentation
- README/CHANGELOG/docs/memory banks updated

## Related Commands
- Suggested follow-up commands

Co-Authored-By: Claude <noreply@anthropic.com>
```

**Quality Checks (Conditional):**
- **Python:** `ruff check .` + `ruff format --check .`
- **SQL:** Syntax validation + dangerous operation scan
- **Markdown:** Link validation + code block verification
- **Bash:** shellcheck (if available)

**Database Safety Checks:**
- ‚ö† Schema change detection (schema.sql) ‚Üí recommend backup
- ‚ö† ETL integrity (load_with_staging.py) ‚Üí verify load_tracking
- üö´ Block data files (.csv, .mdb, .accdb)
- üö´ Block credentials (.env, *.key, credentials.*)
- ‚ö† Warn large files (>10MB) ‚Üí suggest Git LFS

**Documentation Updates:**
- **README.md:** Database metrics (size, event count, date coverage)
- **CHANGELOG.md:** Conventional changelog entries
- **CLAUDE.local.md:** Current state, metrics, decisions
- **Cross-References:** Validate all internal links

**Output:**
- Quality check results (pass/fail with actionable errors)
- Staged file summary (`git diff --cached --stat`)
- Comprehensive commit message preview
- Post-commit recommendations (e.g., "Run /validate-schema")

**Performance:**
- **Doc-only changes:** 1-2 minutes (skips code quality checks)
- **Code changes:** 5-10 minutes (full quality validation)
- **Schema changes:** 10-15 minutes (includes safety checks)

**Integration:**
- Use before `/daily-log` to ensure clean commit history
- Combine with `/backup-db` before risky schema changes
- Follow with `/validate-schema` or `/benchmark` as recommended

**Examples:**

**Example 1: Documentation Update**
```bash
/stage-commit

# Detects only .md changed ‚Üí skips Python/SQL validation
# Updates memory banks ‚Üí stages files ‚Üí creates commit

Result:
docs(README): update database statistics with Sprint 2 metrics

Updated row counts, date coverage, and database size
after Pre2008.mdb integration.
```

**Example 2: Schema Change**
```bash
/stage-commit

‚ö† CRITICAL: Schema changes detected!
‚ö† Recommend backup: /backup-db schema
‚ö† Validation: Run /validate-schema after commit

Result:
schema(events): add composite index on (ev_year, ev_state)

Added composite index idx_events_year_state to optimize
year-by-state queries. 10x speedup on aggregations.

## Database State
- Database size: 981 MB (was 966 MB)
- Tables affected: events (new index)

## Related Commands
- Run /validate-schema to verify integrity
- Run /benchmark to measure performance impact
```

**Example 3: ETL Enhancement**
```bash
/stage-commit

# Python linting: PASSED
# Python formatting: PASSED

Result:
etl(loader): improve duplicate detection performance by 50%

Replaced nested loops with set-based operations.
Reduces Pre2008.mdb load time from 180s to 90s.

## Testing/Validation
- Python linting: passed (ruff)
- Load tested: yes (both databases)
```

**Adapted From:** ProRT-IP `/stage-commit` command
**Key Differences:**
- Replaced Rust quality checks with database-specific checks
- Added database safety features (schema detection, ETL validation)
- Enhanced with database metrics in commit messages
- Database-first commit types (data, schema, query, etl)
- "NO SUDO" principle enforcement

---

## EXISTING COMMANDS (Brief Reference)

### 7. `/validate-schema` - Database Validation

Comprehensive database integrity checks with 10 validation phases.

**Usage:** `/validate-schema`, `/validate-schema quick`, `/validate-schema report`

**Validates:** Row counts, primary keys, NULL values, data integrity, foreign keys, indexes, load tracking, database size

---

### 8. `/benchmark` - Query Performance Testing

Measure query performance with p50, p95, p99 latencies.

**Usage:** `/benchmark`, `/benchmark quick`, `/benchmark views`

**Targets:** p50 <10ms, p95 <100ms, p99 <500ms

---

### 9. `/cleanup-staging` - Database Maintenance

Clear staging tables, vacuum database, analyze tables.

**Usage:** `/cleanup-staging`, `/cleanup-staging staging`, `/cleanup-staging vacuum`, `/cleanup-staging full`

---

### 10. `/refresh-mvs` - Materialized View Refresh

Refresh all 6 materialized views concurrently.

**Usage:** `/refresh-mvs`, `/refresh-mvs quick`, `/refresh-mvs <view_name>`

---

### 11. `/sprint-status` - Project Overview

Display current sprint, database metrics, pending tasks.

**Usage:** `/sprint-status`, `/sprint-status quick`, `/sprint-status db`

---

### 12. `/daily-log` - End-of-Day Consolidation

Comprehensive daily log with temporary file preservation (80 min process, 10-20 page report).

**Usage:** `/daily-log`

---

## COMMON WORKFLOWS

### Complete Data Load Workflow ‚≠ê NEW

```bash
# 1. Check current coverage
/data-coverage

# 2. Load new data
/load-data avall.mdb

# 3. Quality assessment
/data-quality

# 4. Create backup
/backup-db --verify --keep 7

# 5. Performance test
/benchmark quick

# 6. Document
/daily-log
```

### Daily Development Routine ‚≠ê UPDATED

```bash
# Morning
/sprint-status                  # Check project state

# Development work
# ... make changes ...

# Quality checks
/validate-schema quick          # Quick validation
/benchmark quick                # Performance check

# Commit changes
/stage-commit                   # Comprehensive pre-commit workflow ‚≠ê NEW

# End of day
/daily-log                      # Consolidate and preserve
```

### Monthly Update Workflow ‚≠ê NEW

```bash
# 1. Backup current state
/backup-db full --verify --keep 10

# 2. Check current coverage
/data-coverage

# 3. Load monthly update
/load-data avall.mdb

# 4. Quality assessment
/data-quality --html

# 5. Performance validation
/benchmark

# 6. Export sample for testing
/export-sample small --recent

# 7. Document changes
/daily-log
```

### Testing/Demo Preparation ‚≠ê NEW

```bash
# 1. Export sample dataset
/export-sample small --year 2020-2023 --state CA,TX,FL

# 2. Extract archive
cd /tmp/NTSB_Datasets/exports/
unzip ntsb_sample_*.zip

# 3. Test with sample data
# Load into test environment, run analysis, etc.
```

### Data Quality Monitoring ‚≠ê NEW

```bash
# Weekly quality checks
/data-quality --html

# Review HTML dashboard
firefox /tmp/NTSB_Datasets/data_quality_*/data_quality_dashboard.html

# Address issues
# - Fix outliers
# - Investigate duplicates
# - Improve completeness
```

### Disaster Recovery ‚≠ê NEW

```bash
# Regular backups (cron job)
0 2 * * * cd /home/parobek/Code/NTSB_Datasets && /backup-db full --keep 7

# Upload to cloud
aws s3 cp backups/ntsb_aviation_full_*.sql.gz s3://my-backups/ntsb/

# Restore if needed
# /restore-db backups/ntsb_aviation_full_TIMESTAMP.sql.gz
```

### Pre-Commit Workflow ‚≠ê NEW

```bash
# After making changes, before committing

# 1. Comprehensive pre-commit workflow
/stage-commit

# Workflow will:
# - Analyze changes (git status, git diff)
# - Run quality checks (Python: ruff, SQL: syntax, Markdown: links)
# - Verify environment (Python venv, PostgreSQL)
# - Check .gitignore (block sensitive files, data files)
# - Detect schema/ETL changes (recommend backups)
# - Update documentation (README, CHANGELOG, memory banks)
# - Validate cross-references
# - Stage all files (git add -A)
# - Create comprehensive commit message
# - Ask for user confirmation
# - Suggest follow-up commands (e.g., /validate-schema)

# Example commit messages created:
# - docs(README): update database statistics
# - schema(events): add composite index for performance
# - etl(loader): improve duplicate detection by 50%
# - data(monthly): load November 2025 NTSB update
```

---

## COMMAND DESIGN PRINCIPLES

### Database-First Design

All commands designed specifically for database projects:
- PostgreSQL-native operations (psql, pg_dump)
- ETL workflow support (staging, loading, validation)
- Analytics focus (materialized views, coverage analysis)
- Data quality emphasis (validation, outlier detection)

### Safety and Reliability

- Confirmation prompts for destructive operations
- Load tracking to prevent duplicate loads
- Comprehensive error handling
- Atomic operations (succeed completely or fail cleanly)
- Backup before major operations

### Comprehensive Documentation

- Every command has 100+ line documentation
- Usage examples with real scenarios
- Troubleshooting sections
- Related commands cross-references
- Output artifacts clearly documented

### Performance Awareness

- Estimated execution times provided
- Performance targets documented
- Optimization recommendations included
- Progress indicators for long operations

---

## INSTALLATION

These commands are automatically available in Claude Code when working in the NTSB_Datasets directory. No installation required.

### Requirements

**Core Tools:**
- `psql` - PostgreSQL client (required for all database commands)
- `pg_dump` - PostgreSQL backup tool (required for /backup-db)
- `git` - Version control (required for /daily-log, /sprint-status, /stage-commit)
- `python` - Python 3 (required for /load-data)
- `ruff` - Python linter/formatter (required for /stage-commit)

**Optional Tools:**
- `mdbtools` - For reading .mdb files directly (optional, /load-data can work without)
- `gzip` - Compression (usually pre-installed)
- `shellcheck` - Bash linter (optional for /stage-commit)

### Installing PostgreSQL Tools

```bash
# Arch Linux
sudo pacman -S postgresql

# Ubuntu/Debian
sudo apt install postgresql-client postgresql-contrib

# macOS
brew install postgresql
```

---

## ERROR HANDLING

All commands include:
- Clear error messages with context
- Troubleshooting guidance
- Actionable next steps
- Safe failures (no data loss)
- Exit codes for scripting

---

## COMMAND SUMMARY TABLE

| Command | Category | Time | Output | Priority | New |
|---------|----------|------|--------|----------|-----|
| `/load-data` | Data Ops | 5-15 min | Load report | HIGH | ‚≠ê |
| `/data-coverage` | Data Ops | 5-10 min | Coverage report + GeoJSON | HIGH | ‚≠ê |
| `/export-sample` | Data Ops | 2-5 min | ZIP archive | MEDIUM | ‚≠ê |
| `/validate-schema` | Quality | 5-10 min | Validation report | HIGH | - |
| `/data-quality` | Quality | 5-10 min | Quality report + HTML | HIGH | ‚≠ê |
| `/backup-db` | Maintenance | 2-5 min | Compressed backup | HIGH | ‚≠ê |
| `/cleanup-staging` | Maintenance | 5-10 min | Space saved report | MEDIUM | - |
| `/refresh-mvs` | Maintenance | 1-2 min | Refresh status | MEDIUM | - |
| `/benchmark` | Performance | 5-10 min | Performance report | MEDIUM | - |
| `/stage-commit` | Version Control | 5-15 min | Git commit | HIGH | ‚≠ê |
| `/sprint-status` | Management | <1 min | Status summary | LOW | - |
| `/daily-log` | Management | 80 min | Comprehensive log | HIGH | - |

**Priority Guide:**
- **HIGH** - Run regularly (daily or after major changes)
- **MEDIUM** - Run as needed (weekly or after specific operations)
- **LOW** - Run for information (anytime)

---

## SUPPORT

**Documentation:**
- This README
- Individual command files (`.claude/commands/*.md`) - Comprehensive inline docs
- CLAUDE.md - Project-specific guidance
- CLAUDE.local.md - Current project state

**Issues:**
- Create issues in GitHub repository
- Tag with `custom-commands` label

---

## CONTRIBUTING

When adding new custom commands:

1. **Follow Patterns** - Use existing commands as templates
2. **Database Focus** - Ensure commands are relevant to database projects
3. **Comprehensive Docs** - Include purpose, usage, examples, troubleshooting
4. **Error Handling** - Use proper exit codes and error messages
5. **Testing** - Test commands with various scenarios
6. **Update README** - Document in this file

---

## CHANGELOG

### 2025-11-06 - Version Control Integration ‚≠ê

**Added (1 new command):**
- `/stage-commit` - Comprehensive pre-commit workflow with quality checks (adapted from ProRT-IP)

**Total Commands:** 12 (was 11)
**Total Lines:** ~10,700+ lines across all commands (was ~10,000)
**Command Categories:** 7 (was 6)

**Improvements:**
- Complete version control workflow automation
- Database-specific commit types (data, schema, query, etl)
- File-type aware quality checks (Python, SQL, Markdown, Bash)
- Database safety features (schema detection, credential blocking)
- Conventional commits with comprehensive metadata
- Integration with existing workflows

### 2025-11-06 - Major Expansion ‚≠ê

**Added (5 new commands):**
- `/load-data` - Intelligent data loading wrapper
- `/data-coverage` - Comprehensive coverage analysis
- `/export-sample` - Sample data export
- `/data-quality` - Data quality dashboard
- `/backup-db` - Automated database backups

**Total Commands:** 11 (was 6)
**Total Lines:** ~10,000+ lines across all commands (was ~3,000)
**Command Categories:** 6 (was 5)

**Improvements:**
- Complete ETL workflow automation
- Comprehensive data quality monitoring
- Backup and disaster recovery support
- Sample data generation for testing
- Enhanced documentation and examples

### 2025-11-06 - Initial Release

**Added (6 commands):**
- `/validate-schema` - Database validation
- `/benchmark` - Query performance testing
- `/cleanup-staging` - Database maintenance
- `/refresh-mvs` - Materialized view management
- `/sprint-status` - Project overview
- `/daily-log` - End-of-day consolidation

---

**Last Updated:** 2025-11-06
**Command Count:** 12 (6 existing + 6 new)
**Total Lines:** ~10,700+ lines across all commands
**Adapted From:** ProRT-IP custom commands + Database-specific originals
**Status:** Production-ready for NTSB Aviation Database project
