{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Aviation Accident Prediction\n",
    "\n",
    "**Phase 2 Sprint 6-7: Statistical Modeling & ML Preparation**\n",
    "\n",
    "**Objective**: Extract ML-ready features from NTSB Aviation Accident Database\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook transforms raw database records into machine learning features for:\n",
    "- Binary classification (fatal vs non-fatal outcome)\n",
    "- Multi-class classification (injury severity levels)\n",
    "- Cause prediction (finding codes)\n",
    "\n",
    "## Feature Categories\n",
    "\n",
    "1. **Temporal**: Year, month, day of week, season\n",
    "2. **Geographic**: State (one-hot), latitude/longitude, region\n",
    "3. **Aircraft**: Make, model, category, damage level\n",
    "4. **Operational**: Phase of flight, weather, light conditions\n",
    "5. **Crew**: Age bins, total hours, certification level\n",
    "6. **Target Variables**: Fatal outcome, severity level, primary finding code\n",
    "\n",
    "## Database Schema\n",
    "\n",
    "Key tables:\n",
    "- `events`: Master accident records (179,809 events, 1962-2025)\n",
    "- `aircraft`: Aircraft involved\n",
    "- `flight_crew`: Crew information\n",
    "- `findings`: Investigation findings and probable causes\n",
    "\n",
    "## Output\n",
    "\n",
    "- `data/ml_features.parquet`: Engineered features ready for modeling\n",
    "- Feature descriptions and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extensions import connection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Execution date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection\n",
    "\n",
    "Connect to PostgreSQL database and verify access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_db_connection() -> connection:\n    \"\"\"Establish connection to NTSB aviation database.\n\n    Returns:\n        PostgreSQL connection object\n    \"\"\"\n    db_params = {\n        'host': os.getenv('DB_HOST', 'localhost'),\n        'port': os.getenv('DB_PORT', '5432'),\n        'database': os.getenv('DB_NAME', 'ntsb_aviation'),\n        'user': os.getenv('DB_USER', os.getenv('USER', 'parobek')),\n    }\n\n    # Add password if provided\n    password = os.getenv('DB_PASSWORD')\n    if password:\n        db_params['password'] = password\n\n    return psycopg2.connect(**db_params)\n\n# Test connection\nconn = get_db_connection()\ncursor = conn.cursor()\n\n# Verify connection\ncursor.execute(\"SELECT version();\")\ndb_version = cursor.fetchone()[0]\nprint(f\"Connected to: {db_version}\")\n\n# Get table counts\ncursor.execute(\"\"\"\n    SELECT schemaname, relname as tablename, n_live_tup\n    FROM pg_stat_user_tables\n    WHERE schemaname = 'public'\n    ORDER BY n_live_tup DESC\n\"\"\")\ntable_counts = cursor.fetchall()\n\nprint(\"\\nTable row counts:\")\nfor schema, table, count in table_counts[:10]:\n    print(f\"  {table}: {count:,}\")\n\ncursor.close()\nconn.close()\n\nprint(\"\\nDatabase connection verified!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Raw Features from Database\n",
    "\n",
    "Query database to extract features from multiple tables with JOIN operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_database() -> pd.DataFrame:\n",
    "    \"\"\"Extract ML features from NTSB aviation database.\n",
    "    \n",
    "    Joins events, aircraft, flight_crew, and findings tables to create\n",
    "    comprehensive feature set for machine learning.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with raw features (one row per event)\n",
    "    \"\"\"\n",
    "    conn = get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH primary_findings AS (\n",
    "        -- Get first finding (in probable cause) for each event\n",
    "        SELECT DISTINCT ON (ev_id)\n",
    "            ev_id,\n",
    "            finding_code AS primary_finding_code\n",
    "        FROM findings\n",
    "        WHERE cm_inpc = true  -- In probable cause\n",
    "        ORDER BY ev_id, id\n",
    "    ),\n",
    "    primary_crew AS (\n",
    "        -- Get first crew member (pilot-in-command) for each event\n",
    "        SELECT DISTINCT ON (ev_id)\n",
    "            ev_id,\n",
    "            crew_age,\n",
    "            pilot_cert,\n",
    "            pilot_tot_time,\n",
    "            pilot_90_days\n",
    "        FROM flight_crew\n",
    "        WHERE crew_category = 'PLT'  -- Pilot\n",
    "        ORDER BY ev_id, crew_no\n",
    "    ),\n",
    "    primary_aircraft AS (\n",
    "        -- Get first aircraft for each event (or only aircraft if single)\n",
    "        SELECT DISTINCT ON (ev_id)\n",
    "            ev_id,\n",
    "            aircraft_key,\n",
    "            acft_make,\n",
    "            acft_model,\n",
    "            acft_category,\n",
    "            damage,\n",
    "            num_eng,\n",
    "            far_part\n",
    "        FROM aircraft\n",
    "        ORDER BY ev_id, aircraft_key\n",
    "    )\n",
    "    SELECT\n",
    "        -- Event identifiers\n",
    "        e.ev_id,\n",
    "        e.ntsb_no,\n",
    "        \n",
    "        -- Temporal features\n",
    "        e.ev_date,\n",
    "        e.ev_year,\n",
    "        e.ev_month,\n",
    "        e.ev_dow,\n",
    "        EXTRACT(DOW FROM e.ev_date) AS day_of_week,\n",
    "        CASE \n",
    "            WHEN e.ev_month IN (12, 1, 2) THEN 'Winter'\n",
    "            WHEN e.ev_month IN (3, 4, 5) THEN 'Spring'\n",
    "            WHEN e.ev_month IN (6, 7, 8) THEN 'Summer'\n",
    "            ELSE 'Fall'\n",
    "        END AS season,\n",
    "        \n",
    "        -- Geographic features\n",
    "        e.ev_state,\n",
    "        e.dec_latitude,\n",
    "        e.dec_longitude,\n",
    "        e.ev_country,\n",
    "        \n",
    "        -- Aircraft features\n",
    "        a.acft_make,\n",
    "        a.acft_model,\n",
    "        a.acft_category,\n",
    "        a.damage AS acft_damage,\n",
    "        a.num_eng,\n",
    "        a.far_part,\n",
    "        \n",
    "        -- Operational features\n",
    "        e.flight_phase,\n",
    "        e.wx_cond_basic,\n",
    "        e.wx_temp,\n",
    "        e.wx_wind_speed,\n",
    "        e.wx_vis,\n",
    "        e.flight_plan_filed,\n",
    "        e.flight_activity,\n",
    "        \n",
    "        -- Crew features\n",
    "        c.crew_age,\n",
    "        c.pilot_cert,\n",
    "        c.pilot_tot_time,\n",
    "        c.pilot_90_days,\n",
    "        \n",
    "        -- Injury/severity features (target variables)\n",
    "        e.ev_highest_injury,\n",
    "        e.inj_tot_f AS total_fatalities,\n",
    "        e.inj_tot_s AS total_serious_injuries,\n",
    "        e.inj_tot_m AS total_minor_injuries,\n",
    "        e.inj_tot_n AS total_uninjured,\n",
    "        CASE WHEN e.inj_tot_f > 0 THEN 1 ELSE 0 END AS fatal_outcome,\n",
    "        \n",
    "        -- Finding/cause features\n",
    "        f.primary_finding_code\n",
    "        \n",
    "    FROM events e\n",
    "    LEFT JOIN primary_aircraft a ON e.ev_id = a.ev_id\n",
    "    LEFT JOIN primary_crew c ON e.ev_id = c.ev_id\n",
    "    LEFT JOIN primary_findings f ON e.ev_id = f.ev_id\n",
    "    WHERE e.ev_year >= 1982  -- Consistent schema after 1982\n",
    "    ORDER BY e.ev_date\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing feature extraction query...\")\n",
    "    print(\"This may take 30-60 seconds for 179,809 events...\")\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\nExtracted {len(df):,} events\")\n",
    "    print(f\"Features: {len(df.columns)}\")\n",
    "    print(f\"Date range: {df['ev_date'].min()} to {df['ev_date'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract raw features\n",
    "raw_df = extract_features_from_database()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment\n",
    "\n",
    "Examine missing values, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"Data types:\")\n",
    "print(raw_df.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values:\")\n",
    "missing = raw_df.isnull().sum()\n",
    "missing_pct = (missing / len(raw_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percent': missing_pct\n",
    "}).sort_values('Percent', ascending=False)\n",
    "\n",
    "# Show features with >5% missing\n",
    "print(missing_df[missing_df['Percent'] > 5])\n",
    "\n",
    "# Basic statistics for numeric columns\n",
    "print(\"\\nNumeric feature statistics:\")\n",
    "numeric_cols = raw_df.select_dtypes(include=[np.number]).columns\n",
    "raw_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "### 5.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute or flag missing values.\n",
    "    \n",
    "    Strategy:\n",
    "    - Categorical: Fill with 'UNKNOWN'\n",
    "    - Numeric: Fill with median or create missing flag\n",
    "    - Geographic: Flag missing coordinates\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with missing values handled\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Categorical features - fill with 'UNKNOWN'\n",
    "    categorical_cols = [\n",
    "        'ev_state', 'acft_make', 'acft_model', 'acft_category', 'acft_damage',\n",
    "        'flight_phase', 'wx_cond_basic', 'pilot_cert', 'far_part',\n",
    "        'flight_plan_filed', 'flight_activity', 'ev_dow', 'season'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('UNKNOWN')\n",
    "    \n",
    "    # Numeric features - fill with median or 0\n",
    "    numeric_impute = {\n",
    "        'crew_age': df['crew_age'].median(),\n",
    "        'pilot_tot_time': 0,  # 0 hours = unknown\n",
    "        'pilot_90_days': 0,\n",
    "        'num_eng': 1,  # Most aircraft are single-engine\n",
    "        'wx_temp': df['wx_temp'].median(),\n",
    "        'wx_wind_speed': df['wx_wind_speed'].median(),\n",
    "        'wx_vis': df['wx_vis'].median(),\n",
    "    }\n",
    "    \n",
    "    for col, value in numeric_impute.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(value)\n",
    "    \n",
    "    # Geographic - create missing flag\n",
    "    df['has_coordinates'] = (\n",
    "        df['dec_latitude'].notna() & df['dec_longitude'].notna()\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Fill lat/lon with 0 (will be excluded from models if flagged)\n",
    "    df['dec_latitude'] = df['dec_latitude'].fillna(0)\n",
    "    df['dec_longitude'] = df['dec_longitude'].fillna(0)\n",
    "    \n",
    "    # Finding code - fill with 'UNKNOWN'\n",
    "    df['primary_finding_code'] = df['primary_finding_code'].fillna('99999')\n",
    "    \n",
    "    print(\"Missing values handled:\")\n",
    "    print(f\"  Categorical features: {len(categorical_cols)} filled with 'UNKNOWN'\")\n",
    "    print(f\"  Numeric features: {len(numeric_impute)} imputed with median/0\")\n",
    "    print(f\"  Geographic flags: has_coordinates created\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "df = handle_missing_values(raw_df)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nRemaining missing values:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Binned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binned_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create binned versions of continuous features.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with binned features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Age bins\n",
    "    df['age_group'] = pd.cut(\n",
    "        df['crew_age'],\n",
    "        bins=[0, 25, 35, 45, 55, 65, 120],\n",
    "        labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Experience bins (total hours)\n",
    "    df['experience_level'] = pd.cut(\n",
    "        df['pilot_tot_time'],\n",
    "        bins=[-1, 100, 500, 1000, 5000, np.inf],\n",
    "        labels=['<100hrs', '100-500hrs', '500-1000hrs', '1000-5000hrs', '5000+hrs']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Recent flight hours (90-day)\n",
    "    df['recent_activity'] = pd.cut(\n",
    "        df['pilot_90_days'],\n",
    "        bins=[-1, 10, 50, 100, np.inf],\n",
    "        labels=['<10hrs', '10-50hrs', '50-100hrs', '100+hrs']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Temperature bins (Fahrenheit)\n",
    "    df['temp_category'] = pd.cut(\n",
    "        df['wx_temp'],\n",
    "        bins=[-np.inf, 32, 60, 80, np.inf],\n",
    "        labels=['Cold', 'Cool', 'Moderate', 'Hot']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Visibility bins (statute miles)\n",
    "    df['visibility_category'] = pd.cut(\n",
    "        df['wx_vis'],\n",
    "        bins=[-1, 1, 3, 10, np.inf],\n",
    "        labels=['Low', 'Moderate', 'Good', 'Excellent']\n",
    "    ).astype(str)\n",
    "    \n",
    "    print(\"Binned features created:\")\n",
    "    print(\"  - age_group (6 bins)\")\n",
    "    print(\"  - experience_level (5 bins)\")\n",
    "    print(\"  - recent_activity (4 bins)\")\n",
    "    print(\"  - temp_category (4 bins)\")\n",
    "    print(\"  - visibility_category (4 bins)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create binned features\n",
    "df = create_binned_features(df)\n",
    "\n",
    "# Show distribution of binned features\n",
    "print(\"\\nAge group distribution:\")\n",
    "print(df['age_group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Encode Aircraft Make (Top 20 + Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_aircraft_make(df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Encode aircraft make as top N + 'Other'.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        top_n: Number of top makes to keep\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded acft_make_grouped\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get top N makes\n",
    "    top_makes = df['acft_make'].value_counts().head(top_n).index.tolist()\n",
    "    \n",
    "    # Create grouped column\n",
    "    df['acft_make_grouped'] = df['acft_make'].apply(\n",
    "        lambda x: x if x in top_makes else 'OTHER'\n",
    "    )\n",
    "    \n",
    "    print(f\"Aircraft make encoding:\")\n",
    "    print(f\"  Top {top_n} makes retained\")\n",
    "    print(f\"  {len(df[df['acft_make_grouped'] == 'OTHER']):,} events grouped as 'OTHER'\")\n",
    "    print(f\"\\nTop 10 makes:\")\n",
    "    print(df['acft_make_grouped'].value_counts().head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Encode aircraft make\n",
    "df = encode_aircraft_make(df, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Encode Finding Codes (Top 30 + Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_finding_codes(df: pd.DataFrame, top_n: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Encode primary finding codes as top N + 'Other'.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        top_n: Number of top codes to keep\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded finding_code_grouped\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get top N finding codes\n",
    "    top_codes = df['primary_finding_code'].value_counts().head(top_n).index.tolist()\n",
    "    \n",
    "    # Create grouped column\n",
    "    df['finding_code_grouped'] = df['primary_finding_code'].apply(\n",
    "        lambda x: x if x in top_codes else 'OTHER'\n",
    "    )\n",
    "    \n",
    "    print(f\"Finding code encoding:\")\n",
    "    print(f\"  Top {top_n} codes retained\")\n",
    "    print(f\"  {len(df[df['finding_code_grouped'] == 'OTHER']):,} events grouped as 'OTHER'\")\n",
    "    print(f\"\\nTop 10 finding codes:\")\n",
    "    print(df['finding_code_grouped'].value_counts().head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Encode finding codes\n",
    "df = encode_finding_codes(df, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Create Damage Severity Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_damage_severity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encode aircraft damage as ordinal severity.\n",
    "    \n",
    "    NTSB damage codes:\n",
    "    - DEST: Destroyed (most severe) = 4\n",
    "    - SUBS: Substantial = 3\n",
    "    - MINR: Minor = 2\n",
    "    - NONE: None = 1\n",
    "    - UNKNOWN: Unknown = 0\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with damage_severity encoded\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    damage_map = {\n",
    "        'DEST': 4,\n",
    "        'SUBS': 3,\n",
    "        'MINR': 2,\n",
    "        'NONE': 1,\n",
    "        'UNKNOWN': 0\n",
    "    }\n",
    "    \n",
    "    df['damage_severity'] = df['acft_damage'].map(damage_map).fillna(0).astype(int)\n",
    "    \n",
    "    print(\"Damage severity encoding:\")\n",
    "    print(df['damage_severity'].value_counts().sort_index())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Encode damage severity\n",
    "df = encode_damage_severity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Create Region from State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_region(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assign US census region based on state.\n",
    "    \n",
    "    Regions:\n",
    "    - Northeast, Midwest, South, West, Other\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with region column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    regions = {\n",
    "        'Northeast': ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'],\n",
    "        'Midwest': ['IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'],\n",
    "        'South': ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'WV', 'AL', 'KY', 'MS', 'TN', \n",
    "                  'AR', 'LA', 'OK', 'TX'],\n",
    "        'West': ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA']\n",
    "    }\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    state_to_region = {}\n",
    "    for region, states in regions.items():\n",
    "        for state in states:\n",
    "            state_to_region[state] = region\n",
    "    \n",
    "    df['region'] = df['ev_state'].map(state_to_region).fillna('Other')\n",
    "    \n",
    "    print(\"Region assignment:\")\n",
    "    print(df['region'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Assign regions\n",
    "df = assign_region(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Create Injury Severity Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_severity_levels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create multi-class injury severity target variable.\n",
    "    \n",
    "    Levels:\n",
    "    - FATAL: Any fatalities\n",
    "    - SERIOUS: Serious injuries but no fatalities\n",
    "    - MINOR: Minor injuries only\n",
    "    - NONE: No injuries\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with severity_level column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def classify_severity(row):\n",
    "        if row['total_fatalities'] > 0:\n",
    "            return 'FATAL'\n",
    "        elif row['total_serious_injuries'] > 0:\n",
    "            return 'SERIOUS'\n",
    "        elif row['total_minor_injuries'] > 0:\n",
    "            return 'MINOR'\n",
    "        else:\n",
    "            return 'NONE'\n",
    "    \n",
    "    df['severity_level'] = df.apply(classify_severity, axis=1)\n",
    "    \n",
    "    print(\"Severity level distribution:\")\n",
    "    print(df['severity_level'].value_counts())\n",
    "    print(f\"\\nFatal rate: {(df['severity_level'] == 'FATAL').mean():.2%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create severity levels\n",
    "df = create_severity_levels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select Final Features for Modeling\n",
    "\n",
    "Choose features to include in ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "feature_groups = {\n",
    "    'temporal': [\n",
    "        'ev_year', 'ev_month', 'day_of_week', 'season'\n",
    "    ],\n",
    "    'geographic': [\n",
    "        'ev_state', 'region', 'dec_latitude', 'dec_longitude', 'has_coordinates'\n",
    "    ],\n",
    "    'aircraft': [\n",
    "        'acft_make_grouped', 'acft_category', 'damage_severity', 'num_eng', 'far_part'\n",
    "    ],\n",
    "    'operational': [\n",
    "        'flight_phase', 'wx_cond_basic', 'temp_category', 'visibility_category',\n",
    "        'flight_plan_filed', 'flight_activity'\n",
    "    ],\n",
    "    'crew': [\n",
    "        'age_group', 'pilot_cert', 'experience_level', 'recent_activity'\n",
    "    ],\n",
    "    'targets': [\n",
    "        'fatal_outcome', 'severity_level', 'finding_code_grouped'\n",
    "    ],\n",
    "    'identifiers': [\n",
    "        'ev_id', 'ntsb_no', 'ev_date'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten feature list\n",
    "all_features = []\n",
    "for group, features in feature_groups.items():\n",
    "    all_features.extend(features)\n",
    "\n",
    "# Select final features\n",
    "ml_df = df[all_features].copy()\n",
    "\n",
    "print(f\"Final feature set: {len(all_features)} features\")\n",
    "print(f\"\\nFeature groups:\")\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"  {group}: {len(features)} features\")\n",
    "\n",
    "print(f\"\\nDataset shape: {ml_df.shape}\")\n",
    "print(f\"Memory usage: {ml_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Statistics and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures directory\n",
    "figures_dir = Path('notebooks/modeling/figures')\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Fatal outcome\n",
    "ml_df['fatal_outcome'].value_counts().plot(\n",
    "    kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "axes[0].set_title('Fatal Outcome Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Fatal Outcome (0=No, 1=Yes)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Severity level\n",
    "ml_df['severity_level'].value_counts().plot(\n",
    "    kind='bar', ax=axes[1], color=sns.color_palette('Set2')\n",
    ")\n",
    "axes[1].set_title('Injury Severity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Severity Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / '01_target_variable_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: 01_target_variable_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with fatal outcome (for categorical features)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Damage severity vs fatal outcome\n",
    "fatal_by_damage = ml_df.groupby('damage_severity')['fatal_outcome'].mean() * 100\n",
    "fatal_by_damage.plot(kind='bar', ax=axes[0, 0], color='#e74c3c')\n",
    "axes[0, 0].set_title('Fatal Rate by Damage Severity', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Damage Severity (0=Unknown, 1=None, 2=Minor, 3=Substantial, 4=Destroyed)')\n",
    "axes[0, 0].set_ylabel('Fatal Rate (%)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Weather condition vs fatal outcome\n",
    "fatal_by_wx = ml_df.groupby('wx_cond_basic')['fatal_outcome'].mean() * 100\n",
    "fatal_by_wx.plot(kind='bar', ax=axes[0, 1], color='#3498db')\n",
    "axes[0, 1].set_title('Fatal Rate by Weather Condition', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Weather Condition')\n",
    "axes[0, 1].set_ylabel('Fatal Rate (%)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Flight phase vs fatal outcome\n",
    "top_phases = ml_df['flight_phase'].value_counts().head(10).index\n",
    "fatal_by_phase = ml_df[ml_df['flight_phase'].isin(top_phases)].groupby('flight_phase')['fatal_outcome'].mean() * 100\n",
    "fatal_by_phase.plot(kind='barh', ax=axes[1, 0], color='#9b59b6')\n",
    "axes[1, 0].set_title('Fatal Rate by Flight Phase (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Fatal Rate (%)')\n",
    "axes[1, 0].set_ylabel('Flight Phase')\n",
    "\n",
    "# Region vs fatal outcome\n",
    "fatal_by_region = ml_df.groupby('region')['fatal_outcome'].mean() * 100\n",
    "fatal_by_region.plot(kind='bar', ax=axes[1, 1], color='#e67e22')\n",
    "axes[1, 1].set_title('Fatal Rate by Region', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Region')\n",
    "axes[1, 1].set_ylabel('Fatal Rate (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / '02_fatal_rate_by_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: 02_fatal_rate_by_features.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Features\n",
    "\n",
    "Save to Parquet format for efficient storage and fast loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if needed\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save to parquet\n",
    "output_path = data_dir / 'ml_features.parquet'\n",
    "ml_df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "\n",
    "print(f\"Features saved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# Also save feature metadata\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'num_samples': len(ml_df),\n",
    "    'num_features': len(all_features),\n",
    "    'date_range': {\n",
    "        'start': ml_df['ev_date'].min().isoformat(),\n",
    "        'end': ml_df['ev_date'].max().isoformat()\n",
    "    },\n",
    "    'feature_groups': {k: len(v) for k, v in feature_groups.items()},\n",
    "    'target_distributions': {\n",
    "        'fatal_outcome': ml_df['fatal_outcome'].value_counts().to_dict(),\n",
    "        'severity_level': ml_df['severity_level'].value_counts().to_dict()\n",
    "    },\n",
    "    'missing_values': ml_df.isnull().sum().to_dict()\n",
    "}\n",
    "\n",
    "metadata_path = data_dir / 'ml_features_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nMetadata saved to: {metadata_path}\")\n",
    "print(\"\\nFeature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Features Created\n",
    "\n",
    "**Total features**: 39 (excluding identifiers)\n",
    "\n",
    "**Feature groups**:\n",
    "- Temporal: 4 features (year, month, day of week, season)\n",
    "- Geographic: 5 features (state, region, lat/lon, coordinate flag)\n",
    "- Aircraft: 5 features (make, category, damage severity, engines, FAR part)\n",
    "- Operational: 6 features (flight phase, weather, temperature, visibility, flight plan, activity)\n",
    "- Crew: 4 features (age group, certification, experience, recent activity)\n",
    "- Targets: 3 variables (fatal outcome, severity level, finding code)\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- **Dataset size**: ~92,000 events (1982-2025)\n",
    "- **Missing values**: Handled via imputation (median, mode, 'UNKNOWN')\n",
    "- **Class balance**: ~10% fatal events (imbalanced, will use class_weight)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Logistic Regression** (Notebook 01): Binary classification for fatal outcome\n",
    "2. **Cox Proportional Hazards** (Notebook 02): Survival analysis\n",
    "3. **Random Forest** (Notebook 03): Multi-class cause classification\n",
    "4. **Model Evaluation** (Notebook 04): Compare all models\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `data/ml_features.parquet`: Engineered features (ready for modeling)\n",
    "- `data/ml_features_metadata.json`: Feature metadata and statistics\n",
    "- `notebooks/modeling/figures/01_target_variable_distribution.png`\n",
    "- `notebooks/modeling/figures/02_fatal_rate_by_features.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}