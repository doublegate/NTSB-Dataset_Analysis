{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "# Geospatial Data Preparation\n",
    "\n",
    "**Notebook**: 00_geospatial_data_preparation.ipynb  \n",
    "**Sprint**: Phase 2 Sprint 8 - Advanced Geospatial Analysis  \n",
    "**Created**: 2025-11-08  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Extract geospatial data from PostgreSQL database\n",
    "2. Validate and clean coordinate data\n",
    "3. Create GeoDataFrame with proper CRS\n",
    "4. Remove statistical outliers\n",
    "5. Prepare data for geospatial analysis\n",
    "6. Save processed dataset\n",
    "\n",
    "## Data Source\n",
    "\n",
    "- **Database**: ntsb_aviation (PostgreSQL 18.0 with PostGIS)\n",
    "- **Table**: events\n",
    "- **Total Events**: 179,809 (1962-2025)\n",
    "- **Expected Coordinates**: ~77,891 events (43.32% coverage)\n",
    "\n",
    "## Output\n",
    "\n",
    "- `data/geospatial_events.parquet` - Clean geospatial dataset\n",
    "- `data/geospatial_events_stats.json` - Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 6)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../../data')\n",
    "FIG_DIR = Path('figures')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('✅ All packages imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db-connection",
   "metadata": {},
   "source": [
    "## 1. Database Connection and Data Extraction\n",
    "\n",
    "Extract events with coordinates and relevant attributes for geospatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "engine = create_engine('postgresql://parobek@localhost/ntsb_aviation')\n",
    "\n",
    "# SQL query - extract events with coordinates\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    ev_id,\n",
    "    ev_date,\n",
    "    ev_year,\n",
    "    ev_state,\n",
    "    ev_city,\n",
    "    ev_site_zipcode,\n",
    "    dec_latitude,\n",
    "    dec_longitude,\n",
    "    inj_tot_f,\n",
    "    inj_tot_s,\n",
    "    inj_tot_m,\n",
    "    inj_tot_n,\n",
    "    acft_damage,\n",
    "    acft_make,\n",
    "    acft_model,\n",
    "    acft_category,\n",
    "    far_part,\n",
    "    flt_plan_filed,\n",
    "    wx_cond_basic,\n",
    "    light_cond\n",
    "FROM events\n",
    "WHERE dec_latitude IS NOT NULL \n",
    "  AND dec_longitude IS NOT NULL\n",
    "  AND dec_latitude BETWEEN -90 AND 90\n",
    "  AND dec_longitude BETWEEN -180 AND 180\n",
    "ORDER BY ev_date;\n",
    "\"\"\"\n",
    "\n",
    "print('Extracting geospatial data from database...')\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "print(f'✅ Extracted {len(df):,} events with valid coordinates')\n",
    "print(f'Date range: {df[\"ev_date\"].min()} to {df[\"ev_date\"].max()}')\n",
    "print(f'Year range: {df[\"ev_year\"].min()} to {df[\"ev_year\"].max()}')\n",
    "print(f'Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-quality",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "Check for missing values, outliers, and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print('\\n=== Missing Values Summary ===')\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Coordinate statistics\n",
    "print('\\n=== Coordinate Statistics ===')\n",
    "print(df[['dec_latitude', 'dec_longitude']].describe())\n",
    "\n",
    "# Fatality statistics\n",
    "total_fatalities = df['inj_tot_f'].sum()\n",
    "total_serious = df['inj_tot_s'].sum()\n",
    "print(f'\\nTotal fatalities: {total_fatalities:,}')\n",
    "print(f'Total serious injuries: {total_serious:,}')\n",
    "print(f'Fatal accidents: {(df[\"inj_tot_f\"] > 0).sum():,} ({(df[\"inj_tot_f\"] > 0).mean()*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier-detection",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection and Removal\n",
    "\n",
    "Use IQR method to identify and remove coordinate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(data: pd.DataFrame, column: str, k: float = 1.5) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Remove outliers using IQR method.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame to process\n",
    "        column: Column name to check for outliers\n",
    "        k: IQR multiplier (1.5 = mild outliers, 3.0 = extreme outliers)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (cleaned DataFrame, number of outliers removed)\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "    \n",
    "    mask = (data[column] >= lower_bound) & (data[column] <= upper_bound)\n",
    "    outliers_removed = (~mask).sum()\n",
    "    \n",
    "    return data[mask], outliers_removed\n",
    "\n",
    "# Remove latitude outliers\n",
    "df_clean, lat_outliers = remove_outliers_iqr(df, 'dec_latitude', k=3.0)\n",
    "print(f'Latitude outliers removed: {lat_outliers}')\n",
    "\n",
    "# Remove longitude outliers\n",
    "df_clean, lon_outliers = remove_outliers_iqr(df_clean, 'dec_longitude', k=3.0)\n",
    "print(f'Longitude outliers removed: {lon_outliers}')\n",
    "\n",
    "total_removed = lat_outliers + lon_outliers\n",
    "print(f'\\nTotal outliers removed: {total_removed} ({total_removed/len(df)*100:.3f}%)')\n",
    "print(f'Clean dataset: {len(df_clean):,} events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geodataframe",
   "metadata": {},
   "source": [
    "## 4. Create GeoDataFrame\n",
    "\n",
    "Convert to GeoDataFrame with proper CRS (EPSG:4326 for WGS84)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-gdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Point geometries\n",
    "geometry = [Point(xy) for xy in zip(df_clean['dec_longitude'], df_clean['dec_latitude'])]\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df_clean,\n",
    "    geometry=geometry,\n",
    "    crs='EPSG:4326'  # WGS84 (lat/lon)\n",
    ")\n",
    "\n",
    "print(f'✅ GeoDataFrame created with {len(gdf):,} events')\n",
    "print(f'CRS: {gdf.crs}')\n",
    "print(f'\\nBounds:')\n",
    "print(f'  Latitude:  {gdf[\"dec_latitude\"].min():.6f} to {gdf[\"dec_latitude\"].max():.6f}')\n",
    "print(f'  Longitude: {gdf[\"dec_longitude\"].min():.6f} to {gdf[\"dec_longitude\"].max():.6f}')\n",
    "\n",
    "# Project to US Albers Equal Area (EPSG:5070) for distance-based analysis\n",
    "gdf_proj = gdf.to_crs('EPSG:5070')\n",
    "print(f'\\n✅ Projected to EPSG:5070 (Albers Equal Area) for spatial analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualizations",
   "metadata": {},
   "source": [
    "## 5. Exploratory Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Coordinate scatter plot (all events)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "gdf.plot(ax=ax, markersize=0.5, alpha=0.3, color='blue')\n",
    "ax.set_title(f'NTSB Aviation Accidents with Coordinates (n={len(gdf):,})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Longitude', fontsize=12)\n",
    "ax.set_ylabel('Latitude', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'coordinate_scatter_all.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: coordinate_scatter_all.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-state-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Event distribution by state (top 20)\n",
    "state_counts = gdf['ev_state'].value_counts().head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "state_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title('Top 20 States by Accident Count (with coordinates)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Number of Accidents', fontsize=12)\n",
    "ax.set_ylabel('State', fontsize=12)\n",
    "ax.invert_yaxis()\n",
    "for i, v in enumerate(state_counts.values):\n",
    "    ax.text(v, i, f' {v:,}', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'state_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: state_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Missing coordinate analysis\n",
    "missing_coords_query = \"\"\"\n",
    "SELECT \n",
    "    ev_year,\n",
    "    COUNT(*) as total_events,\n",
    "    COUNT(dec_latitude) as with_coords,\n",
    "    COUNT(*) - COUNT(dec_latitude) as missing_coords,\n",
    "    ROUND(100.0 * COUNT(dec_latitude) / COUNT(*), 2) as coverage_pct\n",
    "FROM events\n",
    "WHERE ev_year IS NOT NULL\n",
    "GROUP BY ev_year\n",
    "ORDER BY ev_year;\n",
    "\"\"\"\n",
    "\n",
    "missing_df = pd.read_sql(missing_coords_query, engine)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Coverage percentage over time\n",
    "ax1.plot(missing_df['ev_year'], missing_df['coverage_pct'], linewidth=2, color='green', label='Coverage %')\n",
    "ax1.fill_between(missing_df['ev_year'], 0, missing_df['coverage_pct'], alpha=0.3, color='green')\n",
    "ax1.set_title('Coordinate Coverage Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Coverage %', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Events with/without coordinates\n",
    "ax2.bar(missing_df['ev_year'], missing_df['with_coords'], label='With Coordinates', color='steelblue', alpha=0.7)\n",
    "ax2.bar(missing_df['ev_year'], missing_df['missing_coords'], bottom=missing_df['with_coords'], \n",
    "        label='Missing Coordinates', color='coral', alpha=0.7)\n",
    "ax2.set_title('Event Counts by Coordinate Availability', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Year', fontsize=12)\n",
    "ax2.set_ylabel('Number of Events', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'coordinate_coverage_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: coordinate_coverage_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-data",
   "metadata": {},
   "source": [
    "## 6. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet (both CRS versions)\n",
    "output_path = DATA_DIR / 'geospatial_events.parquet'\n",
    "output_path_proj = DATA_DIR / 'geospatial_events_projected.parquet'\n",
    "\n",
    "gdf.to_parquet(output_path)\n",
    "gdf_proj.to_parquet(output_path_proj)\n",
    "\n",
    "print(f'✅ Saved: {output_path} ({output_path.stat().st_size / 1024**2:.2f} MB)')\n",
    "print(f'✅ Saved: {output_path_proj} ({output_path_proj.stat().st_size / 1024**2:.2f} MB)')\n",
    "\n",
    "# Save dataset statistics\n",
    "stats = {\n",
    "    'total_events_db': 179809,\n",
    "    'events_with_coords_raw': len(df),\n",
    "    'outliers_removed': total_removed,\n",
    "    'events_clean': len(gdf),\n",
    "    'coverage_pct': round(len(gdf) / 179809 * 100, 2),\n",
    "    'date_range': {\n",
    "        'min': str(gdf['ev_date'].min()),\n",
    "        'max': str(gdf['ev_date'].max())\n",
    "    },\n",
    "    'year_range': {\n",
    "        'min': int(gdf['ev_year'].min()),\n",
    "        'max': int(gdf['ev_year'].max())\n",
    "    },\n",
    "    'coordinate_bounds': {\n",
    "        'lat_min': float(gdf['dec_latitude'].min()),\n",
    "        'lat_max': float(gdf['dec_latitude'].max()),\n",
    "        'lon_min': float(gdf['dec_longitude'].min()),\n",
    "        'lon_max': float(gdf['dec_longitude'].max())\n",
    "    },\n",
    "    'fatalities': {\n",
    "        'total': int(gdf['inj_tot_f'].sum()),\n",
    "        'fatal_accidents': int((gdf['inj_tot_f'] > 0).sum()),\n",
    "        'fatal_accident_pct': round((gdf['inj_tot_f'] > 0).mean() * 100, 2)\n",
    "    },\n",
    "    'top_states': state_counts.head(10).to_dict(),\n",
    "    'crs': {\n",
    "        'original': str(gdf.crs),\n",
    "        'projected': str(gdf_proj.crs)\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_path = DATA_DIR / 'geospatial_events_stats.json'\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f'✅ Saved: {stats_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Dataset Prepared** ✅\n",
    "\n",
    "- **Total Events**: 179,809 in database\n",
    "- **Events with Coordinates**: {len(gdf):,} ({len(gdf)/179809*100:.2f}%)\n",
    "- **Outliers Removed**: {total_removed}\n",
    "- **Date Range**: {gdf['ev_date'].min()} to {gdf['ev_date'].max()}\n",
    "- **Year Range**: {gdf['ev_year'].min()} to {gdf['ev_year'].max()}\n",
    "- **Total Fatalities**: {gdf['inj_tot_f'].sum():,}\n",
    "- **Fatal Accidents**: {(gdf['inj_tot_f'] > 0).sum():,} ({(gdf['inj_tot_f'] > 0).mean()*100:.2f}%)\n",
    "\n",
    "**Files Created**:\n",
    "- `data/geospatial_events.parquet` - GeoDataFrame (EPSG:4326)\n",
    "- `data/geospatial_events_projected.parquet` - GeoDataFrame (EPSG:5070)\n",
    "- `data/geospatial_events_stats.json` - Statistics\n",
    "- `figures/coordinate_scatter_all.png` - Coordinate scatter plot\n",
    "- `figures/state_distribution.png` - State distribution\n",
    "- `figures/coordinate_coverage_analysis.png` - Coverage analysis\n",
    "\n",
    "**Next Steps**:\n",
    "1. DBSCAN Clustering (01_dbscan_clustering.ipynb)\n",
    "2. Kernel Density Estimation (02_kernel_density_estimation.ipynb)\n",
    "3. Getis-Ord Gi* Analysis (03_getis_ord_gi_star.ipynb)\n",
    "4. Moran's I Autocorrelation (04_morans_i_autocorrelation.ipynb)\n",
    "5. Interactive Visualizations (05_interactive_geospatial_viz.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
