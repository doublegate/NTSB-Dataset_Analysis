{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering Analysis\n",
    "\n",
    "**Notebook**: 01_dbscan_clustering.ipynb  \n",
    "**Sprint**: Phase 2 Sprint 8 - Advanced Geospatial Analysis  \n",
    "**Created**: 2025-11-08  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Apply DBSCAN (Density-Based Spatial Clustering) to identify accident hotspots\n",
    "2. Analyze cluster characteristics (size, fatalities, aircraft types)\n",
    "3. Identify top hotspot clusters\n",
    "4. Visualize clusters on interactive maps\n",
    "5. Analyze temporal evolution of clusters\n",
    "\n",
    "## DBSCAN Parameters\n",
    "\n",
    "- **eps**: 50 km (search radius, converted to radians for Haversine metric)\n",
    "- **min_samples**: 10 (minimum accidents to form a cluster)\n",
    "- **metric**: Haversine (great-circle distance for lat/lon)\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "- Cluster assignments for all events\n",
    "- Cluster statistics and rankings\n",
    "- Interactive Folium map with clusters\n",
    "- Temporal cluster evolution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../../data')\n",
    "FIG_DIR = Path('figures')\n",
    "MAP_DIR = Path('maps')\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "MAP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('✅ All packages imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## 1. Load Geospatial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geospatial data\n",
    "gdf = gpd.read_parquet(DATA_DIR / 'geospatial_events.parquet')\n",
    "\n",
    "# Load statistics\n",
    "with open(DATA_DIR / 'geospatial_events_stats.json', 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(f'✅ Loaded {len(gdf):,} events')\n",
    "print(f'Date range: {stats[\"date_range\"][\"min\"]} to {stats[\"date_range\"][\"max\"]}')\n",
    "print(f'Total fatalities: {stats[\"fatalities\"][\"total\"]:,}')\n",
    "print(f'CRS: {gdf.crs}')\n",
    "\n",
    "# Display sample\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan",
   "metadata": {},
   "source": [
    "## 2. DBSCAN Clustering\n",
    "\n",
    "Apply DBSCAN with Haversine metric to identify spatial clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-dbscan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates\n",
    "coords = gdf[['dec_latitude', 'dec_longitude']].values\n",
    "\n",
    "# Convert to radians for Haversine metric\n",
    "coords_rad = np.radians(coords)\n",
    "\n",
    "# DBSCAN parameters\n",
    "# eps in radians: 50 km / 6371 km (Earth radius) ≈ 0.00784\n",
    "eps_km = 50  # kilometers\n",
    "earth_radius_km = 6371\n",
    "eps_rad = eps_km / earth_radius_km\n",
    "min_samples = 10\n",
    "\n",
    "print(f'DBSCAN Parameters:')\n",
    "print(f'  eps: {eps_km} km ({eps_rad:.6f} radians)')\n",
    "print(f'  min_samples: {min_samples}')\n",
    "print(f'  metric: haversine')\n",
    "print(f'\\nRunning DBSCAN...')\n",
    "\n",
    "# Run DBSCAN\n",
    "db = DBSCAN(eps=eps_rad, min_samples=min_samples, metric='haversine', n_jobs=-1)\n",
    "labels = db.fit_predict(coords_rad)\n",
    "\n",
    "# Add cluster labels to GeoDataFrame\n",
    "gdf['cluster'] = labels\n",
    "\n",
    "# Analyze results\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "n_clustered = len(gdf) - n_noise\n",
    "\n",
    "print(f'\\n✅ DBSCAN Complete')\n",
    "print(f'Number of clusters: {n_clusters}')\n",
    "print(f'Clustered events: {n_clustered:,} ({n_clustered/len(gdf)*100:.2f}%)')\n",
    "print(f'Noise events: {n_noise:,} ({n_noise/len(gdf)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster-stats",
   "metadata": {},
   "source": [
    "## 3. Cluster Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster statistics (exclude noise cluster -1)\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = gdf[gdf['cluster'] == cluster_id]\n",
    "    \n",
    "    # Calculate centroid\n",
    "    centroid_lat = cluster_data['dec_latitude'].mean()\n",
    "    centroid_lon = cluster_data['dec_longitude'].mean()\n",
    "    \n",
    "    # Most common state\n",
    "    dominant_state = cluster_data['ev_state'].mode()[0] if len(cluster_data['ev_state'].mode()) > 0 else 'Unknown'\n",
    "    \n",
    "    # Fatality statistics\n",
    "    total_fatalities = cluster_data['inj_tot_f'].sum()\n",
    "    fatal_accidents = (cluster_data['inj_tot_f'] > 0).sum()\n",
    "    \n",
    "    # Aircraft statistics\n",
    "    top_aircraft = cluster_data['acft_make'].mode()[0] if len(cluster_data['acft_make'].mode()) > 0 else 'Unknown'\n",
    "    \n",
    "    # Temporal statistics\n",
    "    year_min = cluster_data['ev_year'].min()\n",
    "    year_max = cluster_data['ev_year'].max()\n",
    "    \n",
    "    cluster_stats.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': len(cluster_data),\n",
    "        'centroid_lat': centroid_lat,\n",
    "        'centroid_lon': centroid_lon,\n",
    "        'dominant_state': dominant_state,\n",
    "        'total_fatalities': int(total_fatalities),\n",
    "        'fatal_accidents': int(fatal_accidents),\n",
    "        'avg_fatalities_per_accident': total_fatalities / len(cluster_data),\n",
    "        'fatal_accident_rate': fatal_accidents / len(cluster_data),\n",
    "        'top_aircraft_make': top_aircraft,\n",
    "        'year_min': int(year_min),\n",
    "        'year_max': int(year_max),\n",
    "        'year_span': int(year_max - year_min)\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "# Save cluster statistics\n",
    "cluster_df.to_csv(DATA_DIR / 'cluster_statistics.csv', index=False)\n",
    "print(f'✅ Saved cluster statistics to data/cluster_statistics.csv')\n",
    "\n",
    "# Display top 10 clusters by size\n",
    "print('\\n=== Top 10 Clusters by Size ===')\n",
    "print(cluster_df.nlargest(10, 'size')[[\n",
    "    'cluster_id', 'size', 'dominant_state', 'total_fatalities', \n",
    "    'fatal_accidents', 'centroid_lat', 'centroid_lon'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 clusters by fatality count\n",
    "print('\\n=== Top 10 Clusters by Total Fatalities ===')\n",
    "top_fatality_clusters = cluster_df.nlargest(10, 'total_fatalities')[[\n",
    "    'cluster_id', 'size', 'dominant_state', 'total_fatalities', \n",
    "    'fatal_accidents', 'avg_fatalities_per_accident'\n",
    "]]\n",
    "print(top_fatality_clusters)\n",
    "\n",
    "# Top 10 clusters by fatal accident rate\n",
    "print('\\n=== Top 10 Clusters by Fatal Accident Rate ===')\n",
    "top_fatal_rate_clusters = cluster_df[cluster_df['size'] >= 20].nlargest(10, 'fatal_accident_rate')[[\n",
    "    'cluster_id', 'size', 'dominant_state', 'fatal_accident_rate', 'total_fatalities'\n",
    "]]\n",
    "print(top_fatal_rate_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualizations",
   "metadata": {},
   "source": [
    "## 4. Cluster Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-size-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Cluster size distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(cluster_df['size'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Cluster Size Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Cluster Size (number of accidents)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.axvline(cluster_df['size'].median(), color='red', linestyle='--', label=f'Median: {cluster_df[\"size\"].median():.0f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(cluster_df['size'], vert=True)\n",
    "ax2.set_title('Cluster Size Box Plot', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Cluster Size', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'dbscan_cluster_size_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: dbscan_cluster_size_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-fatalities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Cluster fatality analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter: Size vs Fatalities\n",
    "ax1.scatter(cluster_df['size'], cluster_df['total_fatalities'], \n",
    "            s=50, alpha=0.6, c=cluster_df['fatal_accident_rate'], \n",
    "            cmap='YlOrRd', edgecolors='black', linewidth=0.5)\n",
    "ax1.set_title('Cluster Size vs Total Fatalities', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Cluster Size (accidents)', fontsize=12)\n",
    "ax1.set_ylabel('Total Fatalities', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(ax1.collections[0], ax=ax1)\n",
    "cbar.set_label('Fatal Accident Rate', fontsize=10)\n",
    "\n",
    "# Box plot: Fatalities per accident by cluster size category\n",
    "cluster_df['size_category'] = pd.cut(cluster_df['size'], \n",
    "                                       bins=[0, 20, 50, 100, 1000], \n",
    "                                       labels=['Small (≤20)', 'Medium (21-50)', 'Large (51-100)', 'Very Large (>100)'])\n",
    "cluster_df.boxplot(column='avg_fatalities_per_accident', by='size_category', ax=ax2)\n",
    "ax2.set_title('Average Fatalities per Accident by Cluster Size', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Cluster Size Category', fontsize=12)\n",
    "ax2.set_ylabel('Avg Fatalities per Accident', fontsize=12)\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'dbscan_cluster_fatality_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: dbscan_cluster_fatality_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-states",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Clusters by state (top 15 states)\n",
    "state_cluster_counts = cluster_df['dominant_state'].value_counts().head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "state_cluster_counts.plot(kind='bar', ax=ax, color='teal', edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Number of Accident Clusters by State (Top 15)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('State', fontsize=12)\n",
    "ax.set_ylabel('Number of Clusters', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "for i, v in enumerate(state_cluster_counts.values):\n",
    "    ax.text(i, v, f' {v}', ha='center', va='bottom', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'dbscan_clusters_by_state.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: dbscan_clusters_by_state.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-map",
   "metadata": {},
   "source": [
    "## 5. Interactive Cluster Map\n",
    "\n",
    "Create Folium map with color-coded clusters and centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-map",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base map centered on US\n",
    "m = folium.Map(\n",
    "    location=[39.8283, -98.5795],  # Geographic center of contiguous US\n",
    "    zoom_start=4,\n",
    "    tiles='OpenStreetMap'\n",
    ")\n",
    "\n",
    "# Color palette for clusters (top 10 clusters get distinct colors, rest get gray)\n",
    "top_10_clusters = cluster_df.nlargest(10, 'size')['cluster_id'].tolist()\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', \n",
    "          'beige', 'darkblue', 'darkgreen']\n",
    "\n",
    "# Add cluster markers (sample for performance - MarkerCluster for dense areas)\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = gdf[gdf['cluster'] == cluster_id]\n",
    "    \n",
    "    # Get color for this cluster\n",
    "    if cluster_id in top_10_clusters:\n",
    "        color = colors[top_10_clusters.index(cluster_id)]\n",
    "    else:\n",
    "        color = 'gray'\n",
    "    \n",
    "    # Sample events if cluster is large (for performance)\n",
    "    if len(cluster_data) > 100:\n",
    "        cluster_sample = cluster_data.sample(100, random_state=42)\n",
    "    else:\n",
    "        cluster_sample = cluster_data\n",
    "    \n",
    "    # Add markers to MarkerCluster\n",
    "    marker_cluster = MarkerCluster(name=f'Cluster {cluster_id}')\n",
    "    \n",
    "    for idx, row in cluster_sample.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['dec_latitude'], row['dec_longitude']],\n",
    "            radius=3,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=0.6,\n",
    "            popup=f\"Event: {row['ev_id']}<br>Date: {row['ev_date']}<br>Fatalities: {row['inj_tot_f']}\"\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    marker_cluster.add_to(m)\n",
    "\n",
    "# Add cluster centroids with labels\n",
    "for idx, row in cluster_df.nlargest(20, 'size').iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['centroid_lat'], row['centroid_lon']],\n",
    "        icon=folium.Icon(color='black', icon='info-sign'),\n",
    "        popup=f\"\"\"<b>Cluster {row['cluster_id']}</b><br>\n",
    "                  Size: {row['size']} accidents<br>\n",
    "                  State: {row['dominant_state']}<br>\n",
    "                  Fatalities: {row['total_fatalities']}<br>\n",
    "                  Fatal Accidents: {row['fatal_accidents']}<br>\n",
    "                  Years: {row['year_min']}-{row['year_max']}\"\"\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add legend\n",
    "legend_html = f'''<div style=\"position: fixed; \n",
    "                bottom: 50px; right: 50px; width: 250px; height: auto; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:14px; padding: 10px\">\n",
    "                <p><b>DBSCAN Clusters</b></p>\n",
    "                <p>Total Clusters: {n_clusters}<br>\n",
    "                   Clustered Events: {n_clustered:,}<br>\n",
    "                   Noise Events: {n_noise:,}</p>\n",
    "                <p><b>Top 10 Clusters</b> (by size)<br>\n",
    "                   shown with distinct colors</p>\n",
    "                <p><b>Black markers</b>: Cluster centroids<br>\n",
    "                   (top 20 by size)</p>\n",
    "                </div>'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save map\n",
    "map_path = MAP_DIR / 'dbscan_clusters.html'\n",
    "m.save(str(map_path))\n",
    "print(f'✅ Saved interactive map: {map_path}')\n",
    "\n",
    "# Display in notebook\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal",
   "metadata": {},
   "source": [
    "## 6. Temporal Evolution of Clusters\n",
    "\n",
    "Analyze how clusters have changed over decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decade column\n",
    "gdf['decade'] = (gdf['ev_year'] // 10) * 10\n",
    "\n",
    "# Analyze cluster activity by decade\n",
    "temporal_analysis = []\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_data = gdf[gdf['cluster'] == cluster_id]\n",
    "    \n",
    "    for decade in sorted(gdf['decade'].unique()):\n",
    "        decade_data = cluster_data[cluster_data['decade'] == decade]\n",
    "        \n",
    "        if len(decade_data) > 0:\n",
    "            temporal_analysis.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'decade': decade,\n",
    "                'accidents': len(decade_data),\n",
    "                'fatalities': int(decade_data['inj_tot_f'].sum())\n",
    "            })\n",
    "\n",
    "temporal_df = pd.DataFrame(temporal_analysis)\n",
    "\n",
    "# Top 5 clusters by total size\n",
    "top_5_clusters = cluster_df.nlargest(5, 'size')['cluster_id'].tolist()\n",
    "\n",
    "# Plot temporal evolution for top 5 clusters\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for cluster_id in top_5_clusters:\n",
    "    cluster_temporal = temporal_df[temporal_df['cluster_id'] == cluster_id]\n",
    "    cluster_info = cluster_df[cluster_df['cluster_id'] == cluster_id].iloc[0]\n",
    "    \n",
    "    ax.plot(cluster_temporal['decade'], cluster_temporal['accidents'], \n",
    "            marker='o', linewidth=2, markersize=8,\n",
    "            label=f'Cluster {cluster_id} ({cluster_info[\"dominant_state\"]})')\n",
    "\n",
    "ax.set_title('Temporal Evolution of Top 5 Largest Clusters', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Decade', fontsize=12)\n",
    "ax.set_ylabel('Number of Accidents', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'dbscan_temporal_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('✅ Saved: dbscan_temporal_evolution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-geojson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustered GeoDataFrame as GeoJSON\n",
    "gdf_output = gdf[['ev_id', 'ev_date', 'ev_year', 'ev_state', 'dec_latitude', 'dec_longitude',\n",
    "                  'inj_tot_f', 'cluster', 'geometry']].copy()\n",
    "\n",
    "output_path = DATA_DIR / 'dbscan_clusters.geojson'\n",
    "gdf_output.to_file(output_path, driver='GeoJSON')\n",
    "print(f'✅ Saved: {output_path} ({output_path.stat().st_size / 1024**2:.2f} MB)')\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'dbscan_parameters': {\n",
    "        'eps_km': eps_km,\n",
    "        'eps_radians': float(eps_rad),\n",
    "        'min_samples': min_samples,\n",
    "        'metric': 'haversine'\n",
    "    },\n",
    "    'results': {\n",
    "        'total_events': len(gdf),\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_clustered': n_clustered,\n",
    "        'n_noise': n_noise,\n",
    "        'clustered_pct': round(n_clustered / len(gdf) * 100, 2),\n",
    "        'noise_pct': round(n_noise / len(gdf) * 100, 2)\n",
    "    },\n",
    "    'cluster_statistics': {\n",
    "        'avg_cluster_size': float(cluster_df['size'].mean()),\n",
    "        'median_cluster_size': float(cluster_df['size'].median()),\n",
    "        'max_cluster_size': int(cluster_df['size'].max()),\n",
    "        'min_cluster_size': int(cluster_df['size'].min()),\n",
    "        'total_fatalities_in_clusters': int(cluster_df['total_fatalities'].sum())\n",
    "    },\n",
    "    'top_10_clusters_by_size': cluster_df.nlargest(10, 'size')[[\n",
    "        'cluster_id', 'size', 'dominant_state', 'total_fatalities'\n",
    "    ]].to_dict('records'),\n",
    "    'top_10_clusters_by_fatalities': cluster_df.nlargest(10, 'total_fatalities')[[\n",
    "        'cluster_id', 'size', 'dominant_state', 'total_fatalities'\n",
    "    ]].to_dict('records')\n",
    "}\n",
    "\n",
    "summary_path = DATA_DIR / 'dbscan_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f'✅ Saved: {summary_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**DBSCAN Clustering Complete** ✅\n",
    "\n",
    "**Parameters**:\n",
    "- eps: 50 km\n",
    "- min_samples: 10\n",
    "- metric: Haversine\n",
    "\n",
    "**Results**:\n",
    "- Total Clusters: {n_clusters}\n",
    "- Clustered Events: {n_clustered:,} ({n_clustered/len(gdf)*100:.2f}%)\n",
    "- Noise Events: {n_noise:,} ({n_noise/len(gdf)*100:.2f}%)\n",
    "- Average Cluster Size: {cluster_df['size'].mean():.1f} accidents\n",
    "- Largest Cluster: {cluster_df['size'].max()} accidents\n",
    "\n",
    "**Top 3 Clusters by Size**:\n",
    "1. Cluster {cluster_df.nlargest(1, 'size').iloc[0]['cluster_id']}: {cluster_df.nlargest(1, 'size').iloc[0]['size']} accidents ({cluster_df.nlargest(1, 'size').iloc[0]['dominant_state']})\n",
    "2. Cluster {cluster_df.nlargest(2, 'size').iloc[1]['cluster_id']}: {cluster_df.nlargest(2, 'size').iloc[1]['size']} accidents ({cluster_df.nlargest(2, 'size').iloc[1]['dominant_state']})\n",
    "3. Cluster {cluster_df.nlargest(3, 'size').iloc[2]['cluster_id']}: {cluster_df.nlargest(3, 'size').iloc[2]['size']} accidents ({cluster_df.nlargest(3, 'size').iloc[2]['dominant_state']})\n",
    "\n",
    "**Files Created**:\n",
    "- `data/dbscan_clusters.geojson` - Clustered events\n",
    "- `data/cluster_statistics.csv` - Cluster summary stats\n",
    "- `data/dbscan_summary.json` - Analysis summary\n",
    "- `maps/dbscan_clusters.html` - Interactive map\n",
    "- `figures/dbscan_cluster_size_distribution.png`\n",
    "- `figures/dbscan_cluster_fatality_analysis.png`\n",
    "- `figures/dbscan_clusters_by_state.png`\n",
    "- `figures/dbscan_temporal_evolution.png`\n",
    "\n",
    "**Next Steps**:\n",
    "- Kernel Density Estimation (02_kernel_density_estimation.ipynb)\n",
    "- Getis-Ord Gi* Hotspot Analysis (03_getis_ord_gi_star.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
