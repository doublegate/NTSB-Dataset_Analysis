{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis of NTSB Aviation Accident Narratives\n",
    "\n",
    "**Objective**: Extract and analyze the most important terms and phrases from 67,126 aviation accident narrative descriptions using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization.\n",
    "\n",
    "**Dataset**: NTSB Aviation Accident Database (1977-2025, 48 years)\n",
    "\n",
    "**Methods**:\n",
    "- TF-IDF vectorization for unigrams, bigrams, and trigrams\n",
    "- Top 100 most important terms across entire corpus\n",
    "- Per-decade TF-IDF to track language evolution\n",
    "- Word clouds, bar charts, and heatmaps for visualization\n",
    "\n",
    "**Author**: Claude Code (Anthropic)\n",
    "\n",
    "**Date**: 2025-11-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load narrative dataset\n",
    "df = pd.read_parquet('../../data/narratives_dataset.parquet')\n",
    "\n",
    "print(f'Dataset: {len(df):,} narrative records')\n",
    "print(f'Date range: {df[\"ev_year\"].min()} - {df[\"ev_year\"].max()}')\n",
    "print(f'\\nNarrative completeness:')\n",
    "print(f'  narr_accp (accident): {df[\"narr_accp\"].notna().sum():,} ({df[\"narr_accp\"].notna().mean()*100:.1f}%)')\n",
    "print(f'  narr_cause (probable cause): {df[\"narr_cause\"].notna().sum():,} ({df[\"narr_cause\"].notna().mean()*100:.1f}%)')\n",
    "print(f'\\nFatal accidents: {(df[\"inj_tot_f\"] > 0).sum():,} ({(df[\"inj_tot_f\"] > 0).mean()*100:.1f}%)')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize narrative text.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw narrative text\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and normalized text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs and emails\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters (keep spaces, hyphens, and apostrophes)\n",
    "    text = re.sub(r\"[^a-z0-9\\s'-]\", ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine narratives and preprocess\n",
    "df['full_narrative'] = (df['narr_accp'].fillna('') + ' ' + df['narr_cause'].fillna('')).str.strip()\n",
    "df['clean_narrative'] = df['full_narrative'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty narratives\n",
    "df = df[df['clean_narrative'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f'‚úÖ Preprocessed {len(df):,} narratives')\n",
    "print(f'Average narrative length: {df[\"clean_narrative\"].str.split().str.len().mean():.0f} words')\n",
    "print(f'Median narrative length: {df[\"clean_narrative\"].str.split().str.len().median():.0f} words')\n",
    "\n",
    "# Display example\n",
    "print('\\nExample preprocessed narrative:')\n",
    "print(f'Original: {df[\"full_narrative\"].iloc[0][:200]}...')\n",
    "print(f'Cleaned: {df[\"clean_narrative\"].iloc[0][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vectorization (Unigrams, Bigrams, Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),  # Unigrams, bigrams, trigrams\n",
    "    min_df=10,  # Minimum document frequency (appears in at least 10 documents)\n",
    "    max_df=0.7,  # Maximum document frequency (ignore terms in >70% of documents)\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,  # Use log scaling for term frequency\n",
    "    norm='l2'  # L2 normalization\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print('üîÑ Computing TF-IDF matrix...')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_narrative'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f'‚úÖ TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
    "print(f'   Documents: {tfidf_matrix.shape[0]:,}')\n",
    "print(f'   Features: {tfidf_matrix.shape[1]:,}')\n",
    "print(f'   Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top 100 Most Important Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregate TF-IDF scores\n",
    "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "\n",
    "# Get top 100 terms\n",
    "top_indices = tfidf_scores.argsort()[-100:][::-1]\n",
    "top_terms = [(feature_names[i], tfidf_scores[i]) for i in top_indices]\n",
    "\n",
    "# Create DataFrame\n",
    "top_terms_df = pd.DataFrame(top_terms, columns=['term', 'tfidf_score'])\n",
    "top_terms_df['rank'] = range(1, 101)\n",
    "\n",
    "# Categorize n-grams\n",
    "top_terms_df['ngram_type'] = top_terms_df['term'].apply(\n",
    "    lambda x: 'unigram' if len(x.split()) == 1 else ('bigram' if len(x.split()) == 2 else 'trigram')\n",
    ")\n",
    "\n",
    "print('Top 30 Most Important Terms:\\n')\n",
    "print(top_terms_df.head(30).to_string(index=False))\n",
    "\n",
    "print(f'\\nN-gram distribution in top 100:')\n",
    "print(top_terms_df['ngram_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Word Cloud (Top 50 Terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud\n",
    "wordcloud_dict = {term: score for term, score in top_terms[:50]}\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    relative_scaling=0.5,\n",
    "    min_font_size=10\n",
    ").generate_from_frequencies(wordcloud_dict)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Top 50 Most Important Terms in Aviation Accident Narratives (TF-IDF)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/tfidf_wordcloud_top50.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Word cloud saved: figures/tfidf_wordcloud_top50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Chart (Top 30 Terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for top 30 terms\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "colors = top_terms_df.head(30)['ngram_type'].map({\n",
    "    'unigram': '#3498db',\n",
    "    'bigram': '#e74c3c',\n",
    "    'trigram': '#2ecc71'\n",
    "})\n",
    "\n",
    "ax.barh(range(30), top_terms_df.head(30)['tfidf_score'], color=colors)\n",
    "ax.set_yticks(range(30))\n",
    "ax.set_yticklabels(top_terms_df.head(30)['term'], fontsize=10)\n",
    "ax.set_xlabel('TF-IDF Score (Aggregate)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 30 Most Important Terms in Aviation Accident Narratives', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#3498db', label='Unigram'),\n",
    "    Patch(facecolor='#e74c3c', label='Bigram'),\n",
    "    Patch(facecolor='#2ecc71', label='Trigram')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/tfidf_barchart_top30.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Bar chart saved: figures/tfidf_barchart_top30.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Decade TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decade column\n",
    "df['decade'] = (df['ev_year'] // 10) * 10\n",
    "\n",
    "# Get decades with sufficient data\n",
    "decade_counts = df['decade'].value_counts().sort_index()\n",
    "valid_decades = decade_counts[decade_counts >= 100].index.tolist()\n",
    "\n",
    "print(f'Analyzing {len(valid_decades)} decades with sufficient data (‚â•100 narratives):\\n')\n",
    "print(decade_counts[decade_counts >= 100].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-decade TF-IDF\n",
    "decade_tfidf = {}\n",
    "\n",
    "for decade in valid_decades:\n",
    "    decade_texts = df[df['decade'] == decade]['clean_narrative'].tolist()\n",
    "    \n",
    "    vectorizer_decade = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),  # Unigrams and bigrams only for decades\n",
    "        min_df=5,\n",
    "        max_df=0.7,\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    tfidf_decade = vectorizer_decade.fit_transform(decade_texts)\n",
    "    scores_decade = np.asarray(tfidf_decade.sum(axis=0)).ravel()\n",
    "    features_decade = vectorizer_decade.get_feature_names_out()\n",
    "    \n",
    "    # Get top 20 terms for this decade\n",
    "    top_indices_decade = scores_decade.argsort()[-20:][::-1]\n",
    "    top_terms_decade = [(features_decade[i], scores_decade[i]) for i in top_indices_decade]\n",
    "    \n",
    "    decade_tfidf[decade] = top_terms_decade\n",
    "    \n",
    "    print(f'\\n{decade}s (n={len(decade_texts):,} narratives):')\n",
    "    for i, (term, score) in enumerate(top_terms_decade[:10], 1):\n",
    "        print(f'  {i:2d}. {term:25s} {score:8.1f}')\n",
    "\n",
    "print('\\n‚úÖ Per-decade TF-IDF analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Heatmap: Top Terms Across Decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap data\n",
    "# Get unique terms across all decades (top 30 most frequent)\n",
    "all_decade_terms = {}\n",
    "for decade, terms in decade_tfidf.items():\n",
    "    for term, score in terms:\n",
    "        if term not in all_decade_terms:\n",
    "            all_decade_terms[term] = 0\n",
    "        all_decade_terms[term] += score\n",
    "\n",
    "# Get top 30 terms overall\n",
    "top_overall_terms = sorted(all_decade_terms.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "selected_terms = [term for term, _ in top_overall_terms]\n",
    "\n",
    "# Build heatmap matrix\n",
    "heatmap_data = []\n",
    "for term in selected_terms:\n",
    "    row = []\n",
    "    for decade in sorted(valid_decades):\n",
    "        decade_dict = dict(decade_tfidf[decade])\n",
    "        row.append(decade_dict.get(term, 0))\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data,\n",
    "    index=selected_terms,\n",
    "    columns=[f\"{int(d)}s\" for d in sorted(valid_decades)]\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 14))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'TF-IDF Score'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    fmt='.0f'\n",
    ")\n",
    "plt.title('Evolution of Important Terms Across Decades (TF-IDF)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Decade', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Term', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/tfidf_heatmap_decades.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Heatmap saved: figures/tfidf_heatmap_decades.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TF-IDF by Fatal vs Non-Fatal Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fatal outcome flag\n",
    "df['fatal_outcome'] = df['inj_tot_f'] > 0\n",
    "\n",
    "# Compute TF-IDF for fatal and non-fatal separately\n",
    "fatal_texts = df[df['fatal_outcome']]['clean_narrative'].tolist()\n",
    "nonfatal_texts = df[~df['fatal_outcome']]['clean_narrative'].tolist()\n",
    "\n",
    "print(f'Fatal narratives: {len(fatal_texts):,}')\n",
    "print(f'Non-fatal narratives: {len(nonfatal_texts):,}')\n",
    "\n",
    "# Fatal TF-IDF\n",
    "vectorizer_fatal = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "tfidf_fatal = vectorizer_fatal.fit_transform(fatal_texts)\n",
    "scores_fatal = np.asarray(tfidf_fatal.sum(axis=0)).ravel()\n",
    "features_fatal = vectorizer_fatal.get_feature_names_out()\n",
    "top_fatal = sorted(zip(features_fatal, scores_fatal), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Non-fatal TF-IDF\n",
    "vectorizer_nonfatal = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "tfidf_nonfatal = vectorizer_nonfatal.fit_transform(nonfatal_texts)\n",
    "scores_nonfatal = np.asarray(tfidf_nonfatal.sum(axis=0)).ravel()\n",
    "features_nonfatal = vectorizer_nonfatal.get_feature_names_out()\n",
    "top_nonfatal = sorted(zip(features_nonfatal, scores_nonfatal), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print('\\nTop 20 Terms in FATAL Accidents:')\n",
    "for i, (term, score) in enumerate(top_fatal, 1):\n",
    "    print(f'  {i:2d}. {term:25s} {score:8.1f}')\n",
    "\n",
    "print('\\nTop 20 Terms in NON-FATAL Accidents:')\n",
    "for i, (term, score) in enumerate(top_nonfatal, 1):\n",
    "    print(f'  {i:2d}. {term:25s} {score:8.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Fatal accidents\n",
    "fatal_df = pd.DataFrame(top_fatal[:15], columns=['term', 'score'])\n",
    "axes[0].barh(range(15), fatal_df['score'], color='#e74c3c')\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels(fatal_df['term'], fontsize=10)\n",
    "axes[0].set_xlabel('TF-IDF Score', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title(f'FATAL Accidents (n={len(fatal_texts):,})', \n",
    "                  fontsize=12, fontweight='bold', color='#e74c3c')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Non-fatal accidents\n",
    "nonfatal_df = pd.DataFrame(top_nonfatal[:15], columns=['term', 'score'])\n",
    "axes[1].barh(range(15), nonfatal_df['score'], color='#3498db')\n",
    "axes[1].set_yticks(range(15))\n",
    "axes[1].set_yticklabels(nonfatal_df['term'], fontsize=10)\n",
    "axes[1].set_xlabel('TF-IDF Score', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title(f'NON-FATAL Accidents (n={len(nonfatal_texts):,})', \n",
    "                  fontsize=12, fontweight='bold', color='#3498db')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "fig.suptitle('Top Terms: Fatal vs Non-Fatal Aviation Accidents', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/tfidf_fatal_vs_nonfatal.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Comparison chart saved: figures/tfidf_fatal_vs_nonfatal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TF-IDF ANALYSIS SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nüìä Dataset Statistics:')\n",
    "print(f'   Total narratives analyzed: {len(df):,}')\n",
    "print(f'   Date range: {df[\"ev_year\"].min()} - {df[\"ev_year\"].max()} ({df[\"ev_year\"].max() - df[\"ev_year\"].min() + 1} years)')\n",
    "print(f'   Average narrative length: {df[\"clean_narrative\"].str.split().str.len().mean():.0f} words')\n",
    "print(f'   Fatal accidents: {(df[\"fatal_outcome\"]).sum():,} ({(df[\"fatal_outcome\"]).mean()*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüîç TF-IDF Vectorization:')\n",
    "print(f'   Total features extracted: {len(feature_names):,}')\n",
    "print(f'   N-gram range: unigrams, bigrams, trigrams')\n",
    "print(f'   Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%')\n",
    "\n",
    "print(f'\\nüìà Top 10 Most Important Terms (Overall):')\n",
    "for i, (term, score) in enumerate(top_terms[:10], 1):\n",
    "    print(f'   {i:2d}. {term:30s} {score:10.1f}')\n",
    "\n",
    "print(f'\\nüìÖ Decades Analyzed: {len(valid_decades)}')\n",
    "for decade in sorted(valid_decades):\n",
    "    count = decade_counts[decade]\n",
    "    print(f'   {decade}s: {count:,} narratives')\n",
    "\n",
    "print(f'\\nüíæ Visualizations Created:')\n",
    "print(f'   1. Word cloud (top 50 terms)')\n",
    "print(f'   2. Bar chart (top 30 terms)')\n",
    "print(f'   3. Heatmap (terms across decades)')\n",
    "print(f'   4. Fatal vs Non-fatal comparison')\n",
    "\n",
    "print('\\n‚úÖ TF-IDF Analysis Complete!')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top 100 terms to CSV\n",
    "top_terms_df.to_csv('../../data/tfidf_top100_terms.csv', index=False)\n",
    "print('‚úÖ Exported top 100 terms to: data/tfidf_top100_terms.csv')\n",
    "\n",
    "# Export decade analysis\n",
    "decade_results = []\n",
    "for decade in sorted(valid_decades):\n",
    "    for rank, (term, score) in enumerate(decade_tfidf[decade], 1):\n",
    "        decade_results.append({\n",
    "            'decade': f'{decade}s',\n",
    "            'rank': rank,\n",
    "            'term': term,\n",
    "            'tfidf_score': score\n",
    "        })\n",
    "\n",
    "decade_df = pd.DataFrame(decade_results)\n",
    "decade_df.to_csv('../../data/tfidf_by_decade.csv', index=False)\n",
    "print('‚úÖ Exported decade analysis to: data/tfidf_by_decade.csv')\n",
    "\n",
    "print('\\nüéâ All TF-IDF analysis results saved successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
